\documentclass[platex, a4paper]{jsarticle}
\usepackage{commath}
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath, amssymb,amsfonts,amsthm,bm}
\usepackage{booktabs}
\usepackage{listings}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\newcommand{\code}[1]{\lstinline[basicstyle=\ttfamily\color{green!40!black}]|#1|}
\newcommand{\units}[1] {\:\text{#1}}%
\newcommand{\SN}{S$_N$}
\newcommand{\cyclus}{\textsc{Cyclus}\xspace}
\newcommand{\Cyclus}{\cyclus}
\newcommand{\citeme}{\textcolor{red}{CITE}\xspace}
\newcommand{\cycpp}{\code{cycpp}\xspace}
\newcommand{\TODO}[1] {{\color{red}\textbf{TODO: #1}}}%

\newcommand{\comment}[1]{{\color{green}\textbf{#1}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\GP}{\mathpzc{GP}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\LWR}{\mathrm{LWR}}
\newcommand{\FR}{\mathrm{FR}}
\newcommand{\Total}{\mathrm{Total}}
% \newcommand{\argmin}{\mathrm{argmin}}
% \newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\CYCLUS}{\mathrm{Cyclus}}
\newcommand{\DYMOND}{\mathrm{DYMOND}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\stochastic}{\texttt{`stochastic'}\xspace}
\newcommand{\innerprod}{\texttt{`inner-prod'}\xspace}
\newcommand{\allflag}{\texttt{`all'}\xspace}
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}

\global\long\def\T#1{#1^{\top}}
\begin{document}

\title{Hyper-paramter Optimization \\ Using Gaussian Markov Random Field}
\author{Bin Yang, Kohei Watanabe}
\maketitle

\section{Introduction}
GP

$\bm{f}(\bm{x}) \sim \mathcal{N}(\bm{\mu_x},\,\bm{\Sigma_x})$

GMRF

$\bm{f}(\bm{x}) \sim \mathcal{N}(\bm{\mu_x},\,\bm{\Lambda_x}^{-1})$

\section{algebra}

\subsection{Continuous Value Reward pattern}

\begin{table}[htb]
\begin{tabular}{ll} \toprule
Variables & Explanation \\ \toprule
$\bm{y}$ & the true values of $node$ (hidden variable) \\
$\bm{r}_i$ & the reward list of $node_i$ \\
$n_i$ & the length of the reward list of $node_i$ \\
$\gamma_i=\frac{1}{\sigma_i^2}$ & this controls how observation values affect. $\sigma_i^2$ denotes the variance of observed value of $node_i$ \\
$\gamma_0=\frac{1}{\sigma_0^2}$ & $\sigma_0^2$ denotes the variance of prior value of $node_i$ \\
$\alpha$ & the mean of prior value of $node_i$ \\ \bottomrule
\end{tabular}
\end{table}



\begin{eqnarray}
  p(\bm{r}) &:& const \\
  p\left( \bm{y} |\bm{r}\right) &\propto&  p\left( \bm{r} ,\bm{y}\right) \\
    &=& p\left( \bm{y} \right) p\left( \bm{r} |\bm{y}\right) \\
    &=& p\left( \bm{y} \right) \prod_i p\left( \bm{r}_i |y_i \right) \\
  p\left( y_{i} | \alpha, \gamma_0 \right)
    &=& C\exp \left( -\dfrac {\gamma_0} {2} \left( y_{i}-\alpha \right) ^{2}\right) \\
  p(r_{i}^{j} | y_{i}, \gamma)
    &=& C\exp \left( -\dfrac {\gamma} {2} \left( r_{i}^{j}-y_{i}\right) ^{2}\right) \\
  p\left( \bm{r}_i | y_i, \gamma \right)
    &=& C\exp \left( -\dfrac {\gamma} {2}\sum_{j}^J \left( r_{i}^{j}-y_{i}\right) ^{2}\right) \\
    &=& C\exp \left( -\dfrac {\gamma} {2}\sum_{j}\left( {r_{i}^{j}}^2-2r_{i}^{j}y_{i}+y_{i}^{2}\right) \right) \\
    &=& C'\exp \left( -\dfrac {1} {2}\gamma n_{i}\left( y_{i}-\dfrac {\sum _{j}r_{i}^{j}} {n_{i}}\right) ^{2}\right) \\
  p\left( \bm{r}_i ,y_i | \alpha, \gamma_0, \gamma \right) &=& p\left( y_i | \alpha, \gamma_0 \right) p\left( \bm{r}_i | y_i, \gamma \right) \\
    &=& C\exp \left( -\dfrac {1} {2} \gamma_0 \left( y_{i}-\alpha \right) ^{2}\right)
    \exp \left( -\dfrac {1} {2}\gamma n_{i}\left( y_{i}-\dfrac {\sum _{j}r_{i}^{j}} {n_{i}}\right) ^{2}\right) \\
    &=& C'\exp \left( - \dfrac {1} {2} \tilde{\gamma_i} \left( y_{i} - \tilde {\mu_i} \right) ^{2}  \right)
\end{eqnarray}

where
\begin{eqnarray}
  \tilde{\mu_i} &=& \dfrac {\gamma \sum _{j}r_{i}^{j}+\gamma_0\alpha _{0}} {\gamma n_{i}+\gamma _{0}} \\
  \tilde{\gamma_i} &=& \gamma n_{i}+\gamma _{0}
\end{eqnarray}

\begin{table}[htb]
\begin{tabular}{ll} \toprule
Variables & Explanation \\ \toprule
$\bm{K}$ & adjacency matrix of nodes\\
$\bm{k}$ & sum of $\bm{K}$ per row or per col($\bm{k}_{i}= \sum_i K_{ij} = \sum_j K_{ij}$)\\
$d$ & dimension of search space \\
$l_i$ & length of nodes in $i$-th dimension \\
$N=\prod_i l_i$ & total number of nodes \\
$\bm{I}_N$ & identity matrix whose dimension is $N$\\
\bottomrule
\end{tabular}
\end{table}

\begin{eqnarray}
  \bm{A} &=& \bm{B} - \bm{K} + \gamma_y diag(\bm{k})\\
  \bm{A'} &=& \bm{B} - \bm{K} + 2d\gamma_y\bm{I}_{N} \\
   &=&\begin{pmatrix}
  2d\gamma_y + \tilde{\gamma_1} & -\gamma_y   & \cdots & -\gamma_y   & \cdots &\\
  -\gamma_y & 2d\gamma_y + \tilde{\gamma_2} &\cdots &  \cdots &  -\gamma_y  &  \\
  \vdots&\vdots& \ddots &&& \\
  -\gamma_y &\vdots&& \ddots &&& \\
  &-\gamma_y&&& \ddots &&& \\
  &&\cdots&&& 2d\gamma_y + \tilde{\gamma_N} \\
  \end{pmatrix} \\
  \bm{B} &=& \tilde {\gamma_i} \bm{I}_{N} \\
    &=& \begin{pmatrix}
      \tilde{\gamma_1} & 0 & \cdots & \mbox{\Huge O}\\
      0 & \tilde{\gamma_2} & && \\
      \vdots & & \ddots \\
      \mbox{\Huge O}&&& \tilde{\gamma_N}
  \end{pmatrix}
\end{eqnarray}
  %  \left(\bm{y} \\ \tilde{\bm{\mu}} \right)




$\bm{A}=\T{\bm{D}} \bm{D}$, $\bm{z}=\bm{D}\bm{y}$

\begin{eqnarray}
  p( \bm{r}, \bm{y}| \gamma, \gamma_y, \gamma_0, \alpha)
    &=& C \exp  \left( - \sum_i \left( \dfrac {\gamma_y} {2} \sum_{j\in{J_i}}
    \dfrac{\left(y_i - y_j \right)^2}{2}
      + \dfrac {\tilde{\gamma_i}} {2} \left( y_{i} - \tilde {\mu_i} \right) ^{2} \right) \right)\\
  E(\bm{y})
    &=& \sum_i \left( \dfrac {\gamma_y} {2} \sum_{j\in{J_i}} \dfrac{\left(y_i - y_j \right)^2}{2}
      + \dfrac {\tilde{\gamma_i}} {2} \left( y_{i} - \tilde {\mu_i} \right) ^{2} \right)\\
  2E(\bm{y})
    &=& \begin{pmatrix} \bm{y} ^\top  \tilde{\bm{\mu}}^\top \end{pmatrix}
        \begin{pmatrix}
            \bm{A} & -\bm{B} \\
            -\bm{B} & \bm{B}
        \end{pmatrix}
        \begin{pmatrix}
          \bm{y}  \\
          \tilde{\bm{\mu}}
        \end{pmatrix} \\
    &=& \bm{y} ^\top \bm{A} \bm{y}  - 2 \tilde{\bm{\mu}} \bm{B} \bm{y}
        + \tilde{\bm{\mu}}^\top \bm{B} \tilde{\bm{\mu}} \\
    &=& \bm{y} ^\top \T{\bm{D}} \bm{D} \bm{y}  - 2 \tilde{\bm{\mu}} \bm{B} \bm{y}
        + \tilde{\bm{\mu}}^\top \bm{B} \tilde{\bm{\mu}} \\
    &=& \T{\bm{z}}\bm{z} - 2 \tilde{\bm{\mu}} \bm{B} \bm{D}^{-1}\bm{z}  + \tilde{\bm{\mu}}^\top \bm{B} \tilde{\bm{\mu}} \\
    &=& (\T{\bm{z}} - \T{\tilde{\bm{\mu}}}\bm{B} \bm{D}^{-1}) (\bm{z} - {\bm{D}^{-1}}^\top \bm{B}   \tilde{\bm{\mu}})
    + \tilde{\bm{\mu}}^\top \bm{B} \tilde{\bm{\mu}} - \T{\tilde{\bm{\mu}}}\bm{B} \bm{D}^{-1}
    {\bm{D}^{-1}}^\top \bm{B} \tilde{\bm{\mu}} \\
    &=& (\bm{y} ^\top \T{\bm{D}} - \T{\tilde{\bm{\mu}}}\bm{B} \bm{D}^{-1})(\bm{D}\bm{y}  - {\bm{D}^{-1}}^\top \bm{B} \tilde{\bm{\mu}}) + \tilde{\bm{\mu}}^\top (\bm{B} - \bm{B} \bm{A}^{-1} \bm{B} ) \tilde{\bm{\mu}} \\
    &=&  (\T{\bm{y} } - \T{\tilde{\bm{\mu}}}\bm{B} \bm{D}^{-1} {\bm{D}^{\top}}^{-1}) \T{\bm{D}} \bm{D} (\bm{y}  - \bm{D}^{-1}{\bm{D}^{-1}}^\top \bm{B} \tilde{\bm{\mu}}) + \tilde{\bm{\mu}}^\top (\bm{B} - \bm{B} \bm{A}^{-1} \bm{B} ) \tilde{\bm{\mu}} \\
    &=& (\T{\bm{y} } - \T{\tilde{\bm{\mu}}}\bm{B} \bm{A}^{-1}) \bm{A} (\bm{y}  - \bm{A}^{-1} \bm{B} \tilde{\bm{\mu}})
    + \tilde{\bm{\mu}}^\top (\bm{B} - \bm{B} \bm{A}^{-1} \bm{B} ) \tilde{\bm{\mu}}
\end{eqnarray}

\begin{eqnarray}
  \hat{\bm{y}}
      &=& \argmax_{\bm{y}} \left( \log p(\bm{r},\gamma_y | \bm{y},\gamma, \gamma_0, \alpha) \right)\\
      &=& \T{\tilde{\bm{\mu}}}\bm{B} \bm{A'}^{-1} \\
  \bm{\sigma_{\hat{\bm{y}}}}^2
      &=& diag (\bm{A'}^{-1}) \\
\end{eqnarray}

\begin{eqnarray}
  p( \bm{r}, \bm{y} | \gamma, \gamma_y, \gamma_0, \alpha)
    &=& \int d\bm{y}  p( \bm{r}|\bm{y}, \gamma, \gamma_y, \gamma_0, \alpha ) p(\bm{y} ) \\
    &=&  \int d\bm{y}  \exp\left(- E(\bm{y} ) \right) \\
    &\propto& |\Lambda|^{\frac{1}{2}}  \exp\left( - \frac{1}{2} \tilde{\bm{\mu}}^\top \Lambda \tilde{\bm{\mu}}\right)
\end{eqnarray}

\begin{eqnarray}
  \Lambda &=& \bm{B} - \bm{B} \bm{A}^{-1} \bm{B}\\
           &=& \gamma \bm{A}_2 - \gamma^2 \bm{A}_2 (\gamma_y \bm{A}_1 + \gamma \bm{A}_2) \bm{A}_2
\end{eqnarray}


\subsection{Hyperparamter of GMRF estimation}
\subsubsection{Pairwise Sampling}
\begin{eqnarray*}
\gamma_y &=& \dfrac{1}{td^2} \sum_t (r^t - {r'}^t)^2 \\
\gamma &=& 2d\gamma_y \\
\gamma_0 &=& 0.01 \gamma \\
\alpha &=& \dfrac{1}{t} \sum_t (r^t + {r'}^t) \\
\end{eqnarray*}

\begin{table}[htb]
\begin{tabular}{cl}
${r}^t$&the observed value at time $t$ \\
${r'}^t$&the one of the adjacent observed value at time $t$\\
\end{tabular}
\end{table}

\subsubsection{Maximizing the marginalized loglikelihood}
In order to simplify the problem,
we assume $\gamma_0=0$, then $\tilde{\bm{\mu}_i} = \dfrac {\sum _{j}r_{i}^{j}} {n_{i}}$ becomes $const$.

\begin{eqnarray}
  \bm{A}_1 &=&
    \begin{pmatrix}
      2d & -1 & \cdots & -1 & \cdots &\\
      -1 & 2d &\cdots &  \cdots &  -1  &  \\
      \vdots&\vdots& \ddots &&& \\
      -1 &\vdots&& \ddots &&& \\
      & -1 &&& \ddots &&& \\
      &&&&& 2d \\
    \end{pmatrix} \\
  \bm{A}_2  &=&
    \begin{pmatrix}
      n_1 & 0 & \cdots & \mbox{\Huge O}\\
      0 & n_2 & && \\
      \vdots & & \ddots \\
      \mbox{\Huge O}&&& n_N
    \end{pmatrix}
\end{eqnarray}

\begin{eqnarray}
  A &=& \gamma_y \bm{A}_1 + \gamma \bm{A}_2 \\
  \Lambda &=& \bm{B} - \bm{B} \bm{A}^{-1} \bm{B}\\
           &=& \gamma \bm{A}_2 - \gamma^2 \bm{A}_2 (\gamma_y \bm{A}_1 + \gamma \bm{A}_2) \bm{A}_2 \\
  \frac{\partial \Lambda}{\partial \gamma_y} &=& - \gamma^2 \frac{\partial}{\partial \gamma_y}
    \left(\bm{A}_2 \bm{A}^{-1} \bm{A}_2  \right) \\
    &=& 2 \gamma^2  \left(\bm{A}_2 \bm{A}^{-1} \bm{A}_1 \bm{A}^{-1} \bm{A}_2  \right)
\end{eqnarray}

\begin{eqnarray}
  \log p(\gamma_y | \bm{r}, \bm{y},\gamma, \gamma_0, \alpha)
      &\propto& \log p( \bm{r}, \bm{y} | \gamma, \gamma_y, \gamma_0, \alpha)p(\gamma_y) \\
      &=& \log p( \bm{r}, \bm{y} | \gamma, \gamma_y, \gamma_0, \alpha) + \log p(\gamma_y) \\
      &=& \frac{1}{2} \log|\Lambda| - \frac{1}{2} \tilde{\bm{\mu}}^\top \Lambda \tilde{\bm{\mu}}
        + \log \gamma_y^{k-1} \frac{e^{-\gamma_y/\theta}}{\Gamma(k) \theta^k} \\
      &=& \frac{1}{2} \log|\Lambda| - \frac{1}{2} \tilde{\bm{\mu}}^\top \Lambda \tilde{\bm{\mu}}
        + (k-1)\log \gamma_y -  \frac{\gamma_y}{\theta} + C \\
  \frac{\partial \log p(\gamma_y | \bm{r}, \bm{y}, \gamma, \gamma_0, \alpha)}{\partial \gamma_y}
      &=& \frac{1}{2} \mathrm{Tr}( \Lambda^{-1} \frac{\partial \Lambda}{\partial \gamma_y})
        + \gamma^2   \tilde{\bm{\mu}}^\top \left(\bm{A}_2 \bm{A}^{-1} \bm{A}_1 \bm{A}^{-1} \bm{A}_2  \right)  \tilde{\bm{\mu}}
        + \frac{k-1}{\gamma_y} - \frac{1}{\theta} \\
      &=& \gamma^2 \mathrm{Tr}( \Lambda^{-1} \bm{A}_2 \bm{A}^{-1} \bm{A}_1 \bm{A}^{-1} \bm{A}_2)
        + \gamma^2  \tilde{\bm{\mu}}^\top \left(\bm{A}_2 \bm{A}^{-1} \bm{A}_1 \bm{A}^{-1} \bm{A}_2  \right)  \tilde{\bm{\mu}}
        + \frac{k-1}{\gamma_y} - \frac{1}{\theta} \\
\end{eqnarray}




\section{Real Value Reward pattern}
\subsection{Acquisition Functions}
The predicted mean is $\mu(y)$ and the predicted standard deviation is $\sigma(y)$

\subsubsection{Upper Confidence Bounce (UCB)}
\begin{eqnarray}
a_{\rm UCB}(y) = \mu(y) + \sqrt{\beta^t} \sigma(y)
\end{eqnarray}

\subsubsection{Thompson Sampling}
\begin{eqnarray}
% X \sim \mathcal{N}(\mu(y),\,\sigma^{y})
a_{\rm TS}(y) = \mathcal{N}(\mu(y),\,\sigma(y))
\end{eqnarray}



\subsubsection{Probability Improvement (PI)}
\begin{eqnarray}
a_{\rm PI}(y) = \Phi(z)
\end{eqnarray}
where $z=\dfrac{\mu(y)-r^{\rm best}-\xi}{\sigma(y)}$, $\xi$ is hyperparamter that prevents baysian optimization from getting too greedy. $\Phi$ returns Gaussian cumulative density function.

\subsubsection{Expected Improvement (EI)}
\begin{eqnarray}
a_{\rm EI}(y) = (\mu(y)-r^{\rm best}-\xi)\Phi(z) + \sigma(y)\phi(z)
\end{eqnarray}
$\xi$ is said to work well when it is set 0.01. $\phi$ returns Gaussian probability density function.


\section{Binary Value Reward pattern}
\begin{eqnarray}
  p(x) &=& \dfrac{n_1 + \alpha_1} {n_1 + \alpha_1 + n_0 + \alpha_0} \\
  f(x) &=& \log \left(\dfrac{p(x)}{1-p(x)} \right)
\end{eqnarray}

$n_1$ denotes the number of click, $n_0$ denotes the number of NOT click

\begin{table}[htb]
\begin{tabular}{ll} \toprule
Variables & Explanation \\ \toprule
$n_1$ & the number of click\\
$n_0$ & the number of NOT click \\
$\alpha_1, \alpha_0$ & prior\\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation}
\subsection{Adjacency Matrix Creation}
In order to create matrix$A$, an adjacency matrix is necessary.
An adjacency matrix can be calculated using Kronecker product.


\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
import scipy.sparse
from scipy.spatial.distance import pdist, squareform

def adj_metric(u, v):
    '''
    give this function to scipy.spatial.distance.dist
    :param u (1d numpy array): one coordinate
    :param v (1d numpy array): one coordinate
    :return: 1 if (u, v) is adj else 0
    '''
    if np.abs(u - v).sum() == 1:
        return 1
    else:
        return 0

def create_adjacency_mat_using_pdist(dim_list):
    meshgrid = np.array(np.meshgrid(*[np.arange(ndim) for ndim in dim_list]))
    X_grid = meshgrid.reshape(meshgrid.shape[0], -1).T
    dist = pdist(X_grid, adj_metric)
    return squareform(dist)


def create_adjacency_mat(dim_list, calc_sparse=True):
    K_func = lambda x: scipy.sparse.csc_matrix(x) if calc_sparse else x

    xp = scipy.sparse if calc_sparse else np
    adj1d_list = [create_adjacency_mat_using_pdist([ndim]) for ndim in dim_list]
    if len(dim_list) == 1:
        return K_func(adj1d_list[0])

    K = xp.kron(adj1d_list[1], xp.eye(dim_list[0])) + xp.kron(xp.eye(dim_list[1]), adj1d_list[0])

    prod = dim_list[0] * dim_list[1]

    for i in range(2, len(dim_list)):
        K = xp.kron(K, xp.eye(dim_list[i])) + xp.kron(xp.eye(prod), adj1d_list[i])
        prod *= dim_list[i]

    return K_func(K)


\end{lstlisting}

\subsection{Inverse matrix$A$ calculation }
$A$ is a symmetric matrix and the inverse matrix of $A$ can be calculated efficiently using cholesky decomposition.

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
from sksparse.cholmod import cholesky
# cov = scipy.sparse.linalg.inv(A) # inefficient
factor = cholesky(A)
cov = scipy.sparse.csc_matrix(factor(np.eye(A.shape[0])))
\end{lstlisting}

\section{Tips}

\end{document}
